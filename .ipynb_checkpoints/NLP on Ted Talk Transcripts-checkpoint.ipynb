{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:50:03.459252Z",
     "start_time": "2018-06-18T10:50:03.448132Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T11:46:51.830488Z",
     "start_time": "2018-06-17T11:46:50.550036Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the excel file\n",
    "df = pd.read_excel('data/all_with_liwc_segmented.xls', index_col=0)\n",
    "# print(df.shape)\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T11:46:51.940146Z",
     "start_time": "2018-06-17T11:46:51.832158Z"
    }
   },
   "outputs": [],
   "source": [
    "# there's a lot of columns I don't want to use yet, create subset\n",
    "df = df[df.columns[:34]]\n",
    "df.drop(['persuasive', 'inspiring', 'unconvincing', \n",
    "         'norm_persuasive', 'norm_inspiring', 'norm_unconvincing',\n",
    "         'music', 'conversation'],\n",
    "        axis=1, inplace=True)\n",
    "# show first entry\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T11:46:51.950859Z",
     "start_time": "2018-06-17T11:46:51.942011Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert the datetime columns from unix timestamp to datetime\n",
    "df['film_date'] = df['film_date'].apply(datetime.datetime.fromtimestamp)\n",
    "df['published_date'] = df['published_date'].apply(datetime.datetime.fromtimestamp)\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T11:46:51.954680Z",
     "start_time": "2018-06-17T11:46:51.952658Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.iloc[0]['ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T11:46:52.517876Z",
     "start_time": "2018-06-17T11:46:51.956372Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Currently ratings, related_talks, and tags are stored as strings.\n",
    "Need to convert them to lists by evaluating the string.\n",
    "'''\n",
    "df['ratings'] = df['ratings'].apply(eval)\n",
    "df['related_talks'] = df['related_talks'].apply(eval)\n",
    "df['tags'] = df['tags'].apply(eval)\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T11:47:15.854712Z",
     "start_time": "2018-06-17T11:46:55.258364Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Instead of having the ratings buried in a column of lists of dictionaries,\n",
    "I want to make each individual rating category a new column.\n",
    "The author of the dataset scraped 14 fixed rating categories from the source\n",
    "and I will use those rating categories as new columns. These will probably\n",
    "end up being my target variables in one way or another.\n",
    "\n",
    "I'll also create a normed column for each rating which is just the count\n",
    "of that rating category divided by the total number of views. This will\n",
    "ensure that the ratings are comparable across talks\n",
    "'''\n",
    "\n",
    "# create list with rating categories for new columns\n",
    "new_cols = sorted([x['name'] for x in df.iloc[0]['ratings']])\n",
    "# create list same as above with 'norm_' in front\n",
    "normed_cols = ['norm_{}'.format(x) for x in new_cols]\n",
    "\n",
    "# create the new cols filled with nans\n",
    "for nc in chain(new_cols, normed_cols):\n",
    "    df[nc] = np.nan\n",
    "\n",
    "# fill the columns with proper rating and normed rating\n",
    "for i in df.index:\n",
    "    for rating in df.loc[i]['ratings']:\n",
    "        df.loc[i, rating['name']] = rating['count']\n",
    "        df.loc[i, 'norm_{}'.format(rating['name'])] = rating['count'] / df.loc[i, 'views']\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T09:48:00.459537Z",
     "start_time": "2018-06-18T09:48:00.456438Z"
    }
   },
   "outputs": [],
   "source": [
    "# start nlp on transcripts\n",
    "transcript = df['transcript'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:53:25.407960Z",
     "start_time": "2018-06-18T10:53:25.402939Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert stop words to a set for speed\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# create the preprocessing function to apply to the transcripts\n",
    "def preprocess(sentence):\n",
    "    # convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # tokenize words and remove punctuation\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w+\\'?\\w*')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    # create lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # remove stop words and lemmatize words\n",
    "    filtered_words = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T10:53:39.700583Z",
     "start_time": "2018-06-18T10:53:27.121423Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_words = transcript.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
